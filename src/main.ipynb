{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\estifo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\estifo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\estifo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\estifo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\estifo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\estifo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\estifo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\estifo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\estifo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\estifo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
            "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
            "   ---------------- ----------------------- 3.4/8.0 MB 20.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 7.1/8.0 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.0/8.0 MB 14.6 MB/s eta 0:00:00\n",
            "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.2/2.2 MB 24.4 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
            "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.6/2.6 MB 25.2 MB/s eta 0:00:00\n",
            "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.0 pillow-11.1.0 pyparsing-3.2.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\estifo\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n",
        "%pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9B3ZfuMH79FZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yss8hmiFBjHZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "taIKnvHJ8G65"
      },
      "outputs": [],
      "source": [
        "#pd.to_datetime(pd.read_csv('/content/DAT_ASCII_EURUSD_M1_2000.csv', delimiter=';').iloc[:,0].str[-6:],unit = 's')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "v9vAxRlta7jj",
        "outputId": "51828bfd-e1d6-4ee5-ba65-8fc0db4a479c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('c:/Users/estifo/Downloads/gamma-capture-project/data/DAT_ASCII_EURUSD_M1_2006.csv', header=None, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Si5f5rRcKq"
      },
      "outputs": [],
      "source": [
        "#data = pd.read_csv('/content/DAT_ASCII_EURUSD_M1_2000.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMXnStHE2sbH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUgdxMI5bARH"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT-Y5pjHfDrw"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4GQlKgrblVl"
      },
      "outputs": [],
      "source": [
        "# Rename columns for clarity\n",
        "data.columns = ['DateTime', 'Open', 'High', 'Low', 'Close', 'Volume']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7fFZ6Bs6tDD"
      },
      "outputs": [],
      "source": [
        "data.tail(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xj1Jfhu6cis"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roNXlj9u3Fbu"
      },
      "outputs": [],
      "source": [
        "data.iloc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7bSE9LbsOq"
      },
      "outputs": [],
      "source": [
        "# Convert 'DateTime' to datetime format\n",
        "data['DateTime'] = pd.to_datetime(data['DateTime'], format='%Y%m%d %H%M%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83kXi3ZS7zaA"
      },
      "outputs": [],
      "source": [
        "# Identify duplicate rows based on the 'DateTime' column\n",
        "duplicates = data[data.duplicated(subset=['DateTime'], keep=False)]\n",
        "\n",
        "# Get the indices of duplicate rows\n",
        "duplicate_indices = duplicates.index.tolist()\n",
        "print(\"Indices of Duplicate Rows (Based on DateTime Column):\")\n",
        "print(duplicate_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScsXWYlq8Uyi"
      },
      "outputs": [],
      "source": [
        "# Could not see any duplicated row\n",
        "data.iloc[308461:308480]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruNo1bc1cBiB"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t03_d0_9bsMC"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjLnnoIjbsJe"
      },
      "outputs": [],
      "source": [
        "data.set_index('DateTime', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIZwkL6lc0ZU"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhLl3mo7c0Wr"
      },
      "outputs": [],
      "source": [
        "# Ensure 'DateTime' is the index\n",
        "data.index = pd.to_datetime(data.index)\n",
        "\n",
        "# Generate a full range of 1-minute timestamps\n",
        "full_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='1T')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxmLQT38c0T4"
      },
      "outputs": [],
      "source": [
        "# Find missing timestamps\n",
        "missing_timestamps = full_range.difference(data.index)\n",
        "\n",
        "# Display missing timestamps\n",
        "print(f\"Missing Timestamps:\\n{missing_timestamps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6olgc_Am9IC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmB8TGvhc0RA"
      },
      "outputs": [],
      "source": [
        "data.iloc[35:45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNCrKaNqc0OP"
      },
      "outputs": [],
      "source": [
        "data.iloc[50:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xac06EJjc0L2"
      },
      "outputs": [],
      "source": [
        "missing_timestamps.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b31Y49uEc0In"
      },
      "outputs": [],
      "source": [
        "# Convert missing timestamps to a DataFrame\n",
        "missing_df = pd.DataFrame(missing_timestamps, columns=['DateTime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlJjDk7EnLzX"
      },
      "outputs": [],
      "source": [
        "missing_df['indicator'] = 1\n",
        "missing_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9JIVTJtnLtW"
      },
      "outputs": [],
      "source": [
        "# Generate a complete time range with 1-minute frequency\n",
        "# Instead of data['DateTime'], use data.index to access the DateTime values\n",
        "full_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='T')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpK-76VAnLqb"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe from the complete time range\n",
        "complete_df = pd.DataFrame(full_range, columns=['DateTime'])\n",
        "\n",
        "# Merge the original dataframe with the complete time range\n",
        "combined_df = pd.merge(complete_df, data, on='DateTime', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzddtffUnLoI"
      },
      "outputs": [],
      "source": [
        "# Merge the missing dataframe with the combined dataframe\n",
        "combined_df = pd.merge(combined_df, missing_df, on='DateTime', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWo1px5cakxZ"
      },
      "outputs": [],
      "source": [
        "combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuuliyR_nLlM"
      },
      "outputs": [],
      "source": [
        "# Fill the 'indicator' column: 0 for non-missing, 1 for missing\n",
        "combined_df['indicator'] = combined_df['indicator'].fillna(0).astype(int)\n",
        "\n",
        "# Sort by DateTime (optional)\n",
        "combined_df = combined_df.sort_values(by='DateTime').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szOgpXsIx5vT"
      },
      "outputs": [],
      "source": [
        "combined_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNrxKArUnLij"
      },
      "outputs": [],
      "source": [
        "combined_df.iloc[35:45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvUXaUkInLf0"
      },
      "outputs": [],
      "source": [
        "combined_df.iloc[50:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In9BJ7-usAHf"
      },
      "outputs": [],
      "source": [
        "sample = combined_df.iloc[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek_Ay4NpsFZ9"
      },
      "outputs": [],
      "source": [
        "# Group by date and sum the indicator column (missing minutes per day)\n",
        "missing_per_day = sample.groupby(sample['DateTime'].dt.date)['indicator'].sum()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(15, 5))\n",
        "missing_per_day.plot(kind='bar', color='red')\n",
        "plt.title('Missing Data Per Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Missing Minutes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olnbXvUMqFzg"
      },
      "outputs": [],
      "source": [
        "# Group by date and sum the indicator column (missing minutes per day)\n",
        "missing_per_day = combined_df.groupby(combined_df['DateTime'].dt.date)['indicator'].sum()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(35, 5))\n",
        "missing_per_day.plot(kind='bar', color='red')\n",
        "plt.title('Missing Data Per Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Missing Minutes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLQ9OA8EgVaS"
      },
      "outputs": [],
      "source": [
        "# prompt: can i identify what times on the friday saturday and sunday i have missing data\n",
        "\n",
        "# Assuming 'combined_df' is the DataFrame with the 'indicator' column (0 for present, 1 for missing) and 'DateTime' index.\n",
        "\n",
        "# Filter for Friday, Saturday, and Sunday\n",
        "combined_df['DayOfWeek'] = combined_df['DateTime'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "weekend_df = combined_df[(combined_df['DayOfWeek'] >= 4)] # 4,5,6 are Friday, Saturday, Sunday\n",
        "\n",
        "\n",
        "# Group by date and sum the indicator to find missing data points on weekends\n",
        "missing_weekend = weekend_df.groupby(weekend_df['DateTime'].dt.date)['indicator'].sum()\n",
        "\n",
        "# Print the dates and number of missing data points for Friday, Saturday, and Sunday\n",
        "for date, missing_count in missing_weekend.items():\n",
        "    print(f\"Date: {date}, Missing Count: {missing_count}\")\n",
        "\n",
        "# Alternatively, to get the specific times:\n",
        "for date, missing_count in missing_weekend.items():\n",
        "    if missing_count > 0:\n",
        "        missing_times = weekend_df[(weekend_df['DateTime'].dt.date == date) & (weekend_df['indicator'] == 1)]\n",
        "        print(f\"\\nMissing data on {date}:\")\n",
        "        print(missing_times['DateTime'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fw_QXYfqFuH"
      },
      "outputs": [],
      "source": [
        "missing_per_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkent-SnqFrJ"
      },
      "outputs": [],
      "source": [
        "# Group by month and sum the indicator column (missing minutes per month)\n",
        "missing_per_month = combined_df.groupby(combined_df['DateTime'].dt.month)['indicator'].sum()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(35, 5))\n",
        "missing_per_month.plot(kind='bar', color='red')\n",
        "plt.title('Missing Data Per Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Missing Minutes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncixNCsCrbU2"
      },
      "outputs": [],
      "source": [
        "# We expect to have 4 weekends/ mo. Therefore 4*2*24*60 minutes missing per month = 11,520"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG_j-iGDCR4e"
      },
      "outputs": [],
      "source": [
        "print(missing_per_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3aL3ha6CR1_"
      },
      "outputs": [],
      "source": [
        "sum(missing_per_month)-(11520*12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sss8p5cRCRzV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7i7dHYRCRwX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no6gmGXyCRtv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtGCkgLlCRqr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKDLAvnT102i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNhLZY3B10y9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ahLOYg-10rh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PIxXaDC10op"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ykRu8FCcxEY"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "# Rename columns for clarity\n",
        "data.columns = ['Raw_DateTime', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "# Split the Raw_DateTime column into Date and Time\n",
        "data[['Date', 'Time']] = data['Raw_DateTime'].str.split(' ', expand=True)\n",
        "data = data.drop(columns=['Raw_DateTime'])\n",
        "\n",
        "# Combine Date and Time into a proper Datetime column\n",
        "data['Datetime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'], format='%Y%m%d %H%M%S')\n",
        "data = data.drop(columns=['Date', 'Time'])\n",
        "\n",
        "# Sort by the Datetime column\n",
        "data = data.sort_values(by='Datetime').reset_index(drop=True)\n",
        "\n",
        "# Create a complete 1-minute time range\n",
        "time_range = pd.date_range(start=data['Datetime'].min(), end=data['Datetime'].max(), freq='1T')\n",
        "\n",
        "data.tail(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1i22Ony5wdM"
      },
      "outputs": [],
      "source": [
        "# Find and display duplicate 'Datetime' values\n",
        "duplicate_datetimes = data[data['Datetime'].duplicated(keep=False)]['Datetime']\n",
        "\n",
        "print(\"Duplicate Datetime values:\")\n",
        "print(duplicate_datetimes)\n",
        "\n",
        "# Optionally, you can also display the entire rows with duplicate 'Datetime' values:\n",
        "print(\"\\nRows with duplicate Datetime values:\")\n",
        "print(duplicate_datetimes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbhi2Ozg7Ers"
      },
      "outputs": [],
      "source": [
        "data.iloc[25927:25930]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-Ew2Nm85M_o"
      },
      "outputs": [],
      "source": [
        "#####   review\n",
        "\n",
        "#2\n",
        "# Drop duplicates in 'Datetime' before setting it as the index\n",
        "data = data.drop_duplicates(subset=['Datetime'])\n",
        "# Reindex the dataset to match the complete time range\n",
        "data = data.set_index('Datetime').reindex(time_range).reset_index()\n",
        "data = data.rename(columns={'index': 'Datetime'})\n",
        "\n",
        "# Handle missing values (choose one strategy)\n",
        "# Forward fill missing values\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Backward fill missing values\n",
        "# data.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Display the first few rows of the cleaned dataset\n",
        "data.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeArmoXqf1ME"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the 'Close' prices\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Datetime'], data['Close'], label='Close Price', linewidth=1)\n",
        "\n",
        "# Enhancing the plot aesthetics\n",
        "plt.title('EUR/USD Close Price Over Time', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Close Price', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBLIHCv88EVH"
      },
      "outputs": [],
      "source": [
        "# Create a column to identify hourly groups\n",
        "data['Hour'] = pd.to_datetime(data['Datetime']).dt.floor('h').dt.hour\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnezpL-x-atm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiWtxLR58tmr"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "########    CLOSE-CLOSE RVOL    #######\n",
        "\n",
        "# Compute log returns\n",
        "data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "\n",
        "# Square the log returns\n",
        "data['Squared_Returns'] = data['Log_Returns']**2\n",
        "\n",
        "# Set 'Datetime' as the index before resampling\n",
        "data = data.set_index('Datetime')  # Set 'Datetime' as index\n",
        "\n",
        "# Resample the data to hourly groups and count the number of rows in each group\n",
        "group_sizes = data['Squared_Returns'].resample('h').count() -1 # N-1\n",
        "# Aggregate squared returns by hour (sum)\n",
        "hourly_squared_sum = data['Squared_Returns'].resample('h').sum()\n",
        "hourly_squared_sum\n",
        "# Compute variance by dividing the sum by the group size\n",
        "hourly_variance = hourly_squared_sum / group_sizes\n",
        "hourly_variance\n",
        "# Compute realized volatility as the square root of the variance\n",
        "hourly_rvol = hourly_variance.apply(lambda x: x**0.5)\n",
        "hourly_rvol.tail()\n",
        "\n",
        "# Combine results into a DataFrame for easy analysis\n",
        "hourly_rvol_df = pd.DataFrame({'Hourly_RVol': hourly_rvol})\n",
        "hourly_rvol_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KugTunULAWyw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "hourly_rvol_df['Hourly_RVol'].iloc[-750:-100].plot(kind='line', figsize=(20, 4))\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "plt.title(\"Hourly Realized Volatility (rVol)\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Timestamp\", fontsize=12)\n",
        "plt.ylabel(\"Realized Volatility (rVol)\", fontsize=12)\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAP8h2jbDK0d"
      },
      "outputs": [],
      "source": [
        "#hourly_rvol_df['Hourly_RVol'].iloc[-310:-100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b-OFvJQ91jN"
      },
      "outputs": [],
      "source": [
        "V_ohlc= 0.5*(data[\"High\"]-data[\"Low\"])**2 -(2*np.log(2)-1)*(data[\"Close\"]-data[\"Open\"])**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZXb6bcAuYxm"
      },
      "outputs": [],
      "source": [
        "# Different 60 min rvol/1 min price\n",
        "\n",
        "\n",
        "\n",
        "# Constants\n",
        "k = 2 * np.log(2) - 1  # Constant for Garman-Klass\n",
        "\n",
        "# Garman-Klass Volatility Calculation\n",
        "data['V_GK'] = 0.5 * (np.log(data['High'] / data['Low']) ** 2) - k * (np.log(data['Close'] / data['Open']) ** 2)\n",
        "\n",
        "# Rogers-Satchell Volatility Calculation\n",
        "data['V_RS'] = (np.log(data['High'] / data['Open']) * np.log(data['High'] / data['Close']) +\n",
        "                np.log(data['Low'] / data['Open']) * np.log(data['Low'] / data['Close']))\n",
        "\n",
        "# Close-to-Close Volatility Calculation\n",
        "data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "data['V_CC'] = data['Log_Returns'] ** 2  # Squared log returns for variance\n",
        "\n",
        "# Resampling to hourly intervals\n",
        "hourly_counts = data.resample('H').size()  # Number of data points per hour (N)\n",
        "hourly_data = data.resample('H').agg({\n",
        "    'V_GK': 'sum',\n",
        "    'V_RS': 'sum',\n",
        "    'V_CC': 'sum'\n",
        "})\n",
        "\n",
        "# Adjust by (N-1) for sample variance\n",
        "hourly_data['V_GK'] /= (hourly_counts - 1)\n",
        "hourly_data['V_RS'] /= (hourly_counts - 1)\n",
        "hourly_data['V_CC'] /= (hourly_counts - 1)\n",
        "\n",
        "# Compute realized volatility (square root of sample variances)\n",
        "hourly_data['Hourly_RVol_GK'] = np.sqrt(hourly_data['V_GK'])\n",
        "hourly_data['Hourly_RVol_RS'] = np.sqrt(hourly_data['V_RS'])\n",
        "hourly_data['Hourly_RVol_CC'] = np.sqrt(hourly_data['V_CC'])\n",
        "\n",
        "# Final DataFrame for analysis\n",
        "hourly_rvol_df = hourly_data[['Hourly_RVol_GK', 'Hourly_RVol_RS', 'Hourly_RVol_CC']]\n",
        "# Display the last few rows\n",
        "print(hourly_rvol_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN_n-zyrpfHF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot the data for the specified range\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# Plot Garman-Klass\n",
        "plt.plot(hourly_rvol_df['Hourly_RVol_GK'].iloc[-750:-100], label='Garman-Klass', color='blue')\n",
        "\n",
        "# Plot Rogers-Satchell\n",
        "plt.plot(hourly_rvol_df['Hourly_RVol_RS'].iloc[-750:-100], label='Rogers-Satchell', color='orange')\n",
        "\n",
        "# Plot Close-to-Close\n",
        "plt.plot(hourly_rvol_df['Hourly_RVol_CC'].iloc[-750:-100], label='Close-to-Close', color='green')\n",
        "\n",
        "# Customize the plot\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.title(\"Hourly Realized Volatility (rVol)\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Timestamp\", fontsize=12)\n",
        "plt.ylabel(\"Realized Volatility (rVol)\", fontsize=12)\n",
        "plt.grid(alpha=0.5)\n",
        "\n",
        "# Add legend\n",
        "plt.legend(title=\"Volatility Measure\", fontsize=10)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9WxTD9Yu7YL"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "k = 2 * np.log(2) - 1  # Constant for Garman-Klass\n",
        "\n",
        "# Garman-Klass 1-Minute RVOL\n",
        "data['RVOL_GK_1min'] = np.sqrt(0.5 * (np.log(data['High'] / data['Low']) ** 2) -\n",
        "                          k * (np.log(data['Close'] / data['Open']) ** 2))\n",
        "\n",
        "# Rogers-Satchell 1-Minute RVOL\n",
        "data['RVOL_RS_1min'] = np.sqrt(np.log(data['High'] / data['Open']) * np.log(data['High'] / data['Close']) +\n",
        "                          np.log(data['Low'] / data['Open']) * np.log(data['Low'] / data['Close']))\n",
        "\n",
        "# Close-to-Close 1-Minute RVOL\n",
        "data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "data['RVOL_CC_1min'] = np.sqrt(data['Log_Returns'] ** 2)\n",
        "\n",
        "# Display the last few rows\n",
        "print(data[['Open', 'High', 'Low', 'Close','RVOL_GK_1min', 'RVOL_RS_1min', 'RVOL_CC_1min']].tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ7UAT6vokpu"
      },
      "outputs": [],
      "source": [
        "data[['RVOL_GK_1min', 'RVOL_RS_1min', 'RVOL_CC_1min']].iloc[-124:-100].plot(figsize=(20, 6), title=\"1-Minute Realized Volatility\")\n",
        "plt.grid(alpha=0.5)\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"RVOL\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOTKDmbS6zHg"
      },
      "outputs": [],
      "source": [
        "# Resample 1-minute data into 5-minute OHLC data\n",
        "data_5min = data.resample('5T').agg({\n",
        "    'Open': 'first',\n",
        "    'High': 'max',\n",
        "    'Low': 'min',\n",
        "    'Close': 'last'\n",
        "}).dropna()  # Drop intervals with missing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXVzMGlT607i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Constants\n",
        "k = 2 * np.log(2) - 1  # Constant for Garman-Klass\n",
        "\n",
        "# Garman-Klass Variance Calculation\n",
        "data_5min['V_GK'] = 0.5 * (np.log(data_5min['High'] / data_5min['Low']) ** 2) - \\\n",
        "                    k * (np.log(data_5min['Close'] / data_5min['Open']) ** 2)\n",
        "\n",
        "# Rogers-Satchell Variance Calculation\n",
        "data_5min['V_RS'] = (np.log(data_5min['High'] / data_5min['Open']) * np.log(data_5min['High'] / data_5min['Close']) +\n",
        "                     np.log(data_5min['Low'] / data_5min['Open']) * np.log(data_5min['Low'] / data_5min['Close']))\n",
        "\n",
        "# Close-to-Close Variance Calculation\n",
        "data_5min['Log_Returns'] = np.log(data_5min['Close'] / data_5min['Close'].shift(1))\n",
        "data_5min['V_CC'] = data_5min['Log_Returns'] ** 2\n",
        "\n",
        "# Resample to 360-minute intervals (6 hours)\n",
        "interval = '360T'  # 360 minutes\n",
        "rvol_360min_data = data_5min.resample(interval).agg({\n",
        "    'V_GK': 'sum',\n",
        "    'V_RS': 'sum',\n",
        "    'V_CC': 'sum',\n",
        "    'Log_Returns': 'size'  # Count number of 5-min intervals (N)\n",
        "})\n",
        "\n",
        "# Adjust by (N-1) for sample variance\n",
        "rvol_360min_data['V_GK'] /= (rvol_360min_data['Log_Returns'] - 1)\n",
        "rvol_360min_data['V_RS'] /= (rvol_360min_data['Log_Returns'] - 1)\n",
        "rvol_360min_data['V_CC'] /= (rvol_360min_data['Log_Returns'] - 1)\n",
        "\n",
        "# Compute realized volatility (square root of adjusted variances)\n",
        "rvol_360min_data['RVOL_360min_GK'] = np.sqrt(rvol_360min_data['V_GK'])\n",
        "rvol_360min_data['RVOL_360min_RS'] = np.sqrt(rvol_360min_data['V_RS'])\n",
        "rvol_360min_data['RVOL_360min_CC'] = np.sqrt(rvol_360min_data['V_CC'])\n",
        "\n",
        "# Final DataFrame for analysis\n",
        "rvol_360min_df = rvol_360min_data[['RVOL_360min_GK', 'RVOL_360min_RS', 'RVOL_360min_CC']]\n",
        "\n",
        "# Display the last few rows\n",
        "print(rvol_360min_df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx-KW-3W3Df-"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# Reset the index to make 'Datetime' a regular column\n",
        "data = data.reset_index()\n",
        "\n",
        "# Now perform the join\n",
        "data = data.join(hourly_rvol_df, on=pd.to_datetime(data['Datetime']).dt.floor('H'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoRdd8pI_1On"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Reset the index to make 'Datetime' a regular column\n",
        "data = data.reset_index()\n",
        "\n",
        "# Create a common join key column for hourly_rvol_df\n",
        "data['JoinKey_Hourly'] = pd.to_datetime(data['Datetime']).dt.floor('H')\n",
        "hourly_rvol_df['JoinKey_Hourly'] = hourly_rvol_df.index\n",
        "\n",
        "# Create a common join key column for rvol_360min_df\n",
        "data['JoinKey_360min'] = pd.to_datetime(data['Datetime']).dt.floor('360T')\n",
        "rvol_360min_df['JoinKey_360min'] = rvol_360min_df.index\n",
        "\n",
        "# Now perform the join using the common 'JoinKey' columns\n",
        "data = data.merge(hourly_rvol_df, on='JoinKey_Hourly', how='left').merge(rvol_360min_df, on='JoinKey_360min', how='left')\n",
        "\n",
        "# Optionally, drop the 'JoinKey' columns if not needed\n",
        "data = data.drop(columns=['JoinKey_Hourly', 'JoinKey_360min'])\n",
        "\n",
        "# Set 'Datetime' back as the index\n",
        "# data = data.set_index('Datetime')  # Set 'Datetime' as index (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWGhu7jM8CYC"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rCJQFhZ9R-x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO1otUNr9UGe"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXKC1L3c6wYJ"
      },
      "outputs": [],
      "source": [
        "# Reset the index to a DatetimeIndex if needed\n",
        "data = data.set_index('Datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udSDHQ7yw8j7"
      },
      "outputs": [],
      "source": [
        "data[['Open','High', 'Low', 'Close','RVOL_360min_GK']].iloc[-300:-250]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EqviKuQAo3I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
